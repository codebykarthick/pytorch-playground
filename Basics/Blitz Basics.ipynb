{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e470a72-491c-4e42-a5fb-09ed87ee1a0f",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "This notebook summarizes the content covered in the \"60 Minute Blitz Tutorial\" of PyTorch that covers the basics of PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b85ea-33b3-4cb8-a2ee-dc94c0498478",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "Tensors are basically like stronger numpy arrays. These are special arrays that can do math very quick (needed for neural networks) and can run on CPU, GPU or even specialized TPUs (hardware designed for neural network training from the ground up). In fact tensors and numpy arrays are so alike, they are usually connected through a bridge that allows conversion between the two as they can share the same underlying memory location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e0dc8f-4e33-4919-a7b9-12471c09c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a8e55-a4e9-4dfb-9221-0fc811c62f01",
   "metadata": {},
   "source": [
    "### Tensor Initialization\n",
    "Tensors can be initialized in a ton of different ways, to allow flexibility depending on what the ML pipeline can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ae8361-309e-46e4-899a-43a5b9d81486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From slower native lists\n",
    "data = [[1,2], [3,4]]\n",
    "data_tensor = torch.tensor(data)\n",
    "\n",
    "# From numpy arrays\n",
    "data_numpy = np.array(data)\n",
    "data_tensor_2 = torch.tensor(data)\n",
    "\n",
    "# Similar to numpy they have a bunch of random or fixed value initializations\n",
    "shape = (2, 3,)\n",
    "# Init random values\n",
    "torch.rand(shape)\n",
    "# Init ones\n",
    "torch.ones(shape)\n",
    "# Init zeros\n",
    "torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a52ac1-3bce-4bb9-9e99-88482ed863fb",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "Each tensor that's created has few information associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d31ad23-936d-4ed3-9d86-d1381fb12f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "cpu\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "test = torch.ones(shape)\n",
    "\n",
    "print(test.shape) # dimensions of tensor\n",
    "print(test.device) # where the tensor is stored now, always created on CPU by default\n",
    "print(test.dtype) # Data type of the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc784959-aa94-42de-b6ff-257b2d771704",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "There are some tensor-specific operations that can be performed. Some of them are essentially the same as numpy operations with one exception below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310064e6-a30d-4687-9401-9961ca6578e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "test_gpu = test.to('cuda') # move the tensor to GPU for faster calculation\n",
    "print(test_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f25ae3-5829-47d7-bc4a-6b3e8de7fe48",
   "metadata": {},
   "source": [
    "## AutoGrad\n",
    "\n",
    "AutoGrad is one of the critical components that allow PyTorch to build neural networks. As you might know every neural network consists of two key steps.\n",
    "\n",
    "*Forward Propagation:* Model uses data and its current parameters to make a guess of the end objective (a number for regression, label for classification and so on).\n",
    "\n",
    "*Backward Propagation:* Based on a loss function that tells us how bad our model's guesses were, this tries to get derivatives of the loss function with respect to the parameters to update said parameters by a small value that is opposite to the greatest ascent (hence obviously called gradient descent) multiplied with the learning rate.\n",
    "\n",
    "AutoGrad helps to achieve the derivative needed that makes backward propagation possible for neural networks to \"Learn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3c5a1-eec4-409a-80fe-583ccaa67ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
