{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e470a72-491c-4e42-a5fb-09ed87ee1a0f",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "This notebook summarizes the content covered in the \"60 Minute Blitz Tutorial\" of PyTorch that covers the basics of PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b85ea-33b3-4cb8-a2ee-dc94c0498478",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "Tensors are basically like stronger numpy arrays. These are special arrays that can do math very quick (needed for neural networks) and can run on CPU, GPU or even specialized TPUs (hardware designed for neural network training from the ground up). In fact tensors and numpy arrays are so alike, they are usually connected through a bridge that allows conversion between the two as they can share the same underlying memory location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e0dc8f-4e33-4919-a7b9-12471c09c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a8e55-a4e9-4dfb-9221-0fc811c62f01",
   "metadata": {},
   "source": [
    "### Tensor Initialization\n",
    "Tensors can be initialized in a ton of different ways, to allow flexibility depending on what the ML pipeline can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ae8361-309e-46e4-899a-43a5b9d81486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From slower native lists\n",
    "data = [[1,2], [3,4]]\n",
    "data_tensor = torch.tensor(data)\n",
    "\n",
    "# From numpy arrays\n",
    "data_numpy = np.array(data)\n",
    "data_tensor_2 = torch.tensor(data)\n",
    "\n",
    "# Similar to numpy they have a bunch of random or fixed value initializations\n",
    "shape = (2, 3,)\n",
    "# Init random values\n",
    "torch.rand(shape)\n",
    "# Init ones\n",
    "torch.ones(shape)\n",
    "# Init zeros\n",
    "torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a52ac1-3bce-4bb9-9e99-88482ed863fb",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "Each tensor that's created has few information associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d31ad23-936d-4ed3-9d86-d1381fb12f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "cpu\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "test = torch.ones(shape)\n",
    "\n",
    "print(test.shape) # dimensions of tensor\n",
    "print(test.device) # where the tensor is stored now, always created on CPU by default\n",
    "print(test.dtype) # Data type of the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc784959-aa94-42de-b6ff-257b2d771704",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "There are some tensor-specific operations that can be performed. Some of them are essentially the same as numpy operations with one exception below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310064e6-a30d-4687-9401-9961ca6578e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "test_gpu = test.to('cuda') # move the tensor to GPU for faster calculation\n",
    "print(test_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f25ae3-5829-47d7-bc4a-6b3e8de7fe48",
   "metadata": {},
   "source": [
    "## AutoGrad\n",
    "\n",
    "AutoGrad is one of the critical components that allow PyTorch to build neural networks. As you might know every neural network consists of two key steps.\n",
    "\n",
    "*Forward Propagation:* Model uses data and its current parameters to make a guess of the end objective (a number for regression, label for classification and so on).\n",
    "\n",
    "*Backward Propagation:* Based on a loss function that tells us how bad our model's guesses were, this tries to get derivatives of the loss function with respect to the parameters to update said parameters by a small value that is opposite to the greatest ascent (hence obviously called gradient descent) multiplied with the learning rate.\n",
    "\n",
    "AutoGrad helps to achieve the derivative needed that makes backward propagation possible for neural networks to \"Learn\".\n",
    "\n",
    "Let consider the following polynomial function as a sample loss function.\n",
    "\n",
    "$Q = 3a^3 - b^2$\n",
    "\n",
    "We can see that loss function $Q$ is dependent on two parameters: $a, b$. Therefore the partial derviatives of the function with respect to the two parameters be.\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$ and $\\frac{\\partial Q}{\\partial b} = -2b$\n",
    "\n",
    "### Quirk with PyTorch\n",
    "PyTorch splits the entire forward propagation into two steps. Calling `.backward()` calculates the needed gradients based on the loss function and the model used, while the `.step()` function of the optimizer actually applies the update according to the optimization algorithm used (Gradient descent, ADAM, AdaGrad, etc).\n",
    "\n",
    "Let's check this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5d2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2. ,3.], requires_grad=True)\n",
    "b = torch.tensor([6. ,4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e3c5a1-eec4-409a-80fe-583ccaa67ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Q is a vector and not a scalar, we need to specify the gradient to perform the backward pass\n",
    "Q.backward(gradient=torch.tensor([1., 1.])) # gradient is the same shape as Q and has 1 cause gradient of Q w.r.t Q is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5d326",
   "metadata": {},
   "source": [
    "Now that `.backward()` is invoked, all the gradients are stored in the `.grad` of each of the parameter that was involved in the back prop. We can confirm this by a simple assertion. `.allclose()` is a function provided by PyTorch to see if both values are close enough to be considered equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda3159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(a.grad, 9*a**2) and torch.allclose(b.grad, -2*b) # check if the gradients are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402563c4",
   "metadata": {},
   "source": [
    "### Looks like magic, How does it work?\n",
    "The moment we mark tensors $a \\& b$ as requiring gradient, PyTorch intelligently starts creating a computation graph for them and every tensor that is created because of them, right until the final tensor ($Q$ here). This naturally creates the forward pass where we go from input to loss function.\n",
    "\n",
    "The backward prop kicks off when we call `.backward()` on the root of the computational graph, which computes the gradient for each function and accumulates them in the respective tensors `.grad` property, all the way to the leaf. (Using Chain Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45723191",
   "metadata": {},
   "source": [
    "### When's the fun stuff coming?\n",
    "Right now, actually! We will create a simple *Convolutional Neural Network* using the PyTorch API that can be used to predict digit present in an image. Assume our image is a grayscale image of size 32x32 (single channel). More on their functionality later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2465665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # We define the layers of the network here\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolutions, 0 padding\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass of the network is defined here\n",
    "        # First convolution output\n",
    "        c1 = F.relu(self.conv1(input))\n",
    "        # Max pooling\n",
    "        s2 = F.max_pool2d(c1, (2, 2))\n",
    "        # Second convolution output\n",
    "        c3 = F.relu(self.conv2(s2))\n",
    "        # Second max pooling\n",
    "        s4 = F.max_pool2d(c3, 2)\n",
    "        # Flatten the tensor for dense part of the CNN\n",
    "        s4_flat = torch.flatten(s4, 1)\n",
    "        # Connect the dense layers\n",
    "        f5 = F.relu(self.fc1(s4_flat))\n",
    "        f6 = F.relu(self.fc2(f5))\n",
    "        # Final output layer\n",
    "        output = self.fc3(f6)\n",
    "        return output\n",
    "\n",
    "convNet = ConvNet()\n",
    "print(convNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4dc19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
